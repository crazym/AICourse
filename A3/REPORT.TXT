CSC D84 - Artificial Intelligence

Assignment 3 - Reinforcement Learning - Q Learning

This assignment is worth:

15 AIUs (Artificial Intelligence Units)
toward the 35% assignment component of your final
mark.

________________________________________________

Student Name (last, first): ZHANG, Maoting

Student number: 999590633

UTORid: zhangmao

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

	Signed: _MintyZ__


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (2 marks) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.

      Pretty simple reward logics: award if found a cheese and punish if eaten by cat
      and in general it will be the difference between manhatton distance of cat and cheese.

      Also punish slightly if found a deadend.

2 .- (2 marks each) These are multiple experiments (once you are sure your 
     QLearning code is working!) 

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     # Experiment 1, 100000 training trials, 20 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts):
     
     success rate=0.081634

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate:
     
     Average success rate=0.273204

     # Experiment 2, 10000000 training trials, 50 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts):

     success rate=0.082282

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate:

     Average success rate=0.282899

     Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?     

     No I am afraid not. The success rate (performance) will experience 
     diminishing returns as the number of rounds increase,
     and doesn't seem to reach an optimal value.


     NOTE: I tried the game mode seems the mouse is doing pretty good,
     don't really understand why the success rate remains so low. :P

4 .- (2 marks each) 
     
     Using the QTable saved from Experiment 2 (NO re-training!)

     # Experiment 3 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 4289 for the game.
	
     Mouse Winning Rate:

     Average success rate=0.168636

     # Experiment 4 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 31415 for the game.
	
     Mouse Winning Rate:

     Average success rate=0.175250

     Average rate for Experiement 3 and Experiment 4:

     Compare with the rate obtained in experiment 2. What is happening here?

     The success rate suddenly dropped, indicating that QTable is trained for 
     a specific game environment and does not adpat to changes very well.

5 .- (3 marks)

     # Experiment 5 - Train your mouse on the 16x16 grid, using random seed
     # 1522, 10000000 trials, and 50 training rounds. Cat-smartness=.9

     Initial mouse winning rate (first rate obtained when training starts):

     success rate=0.032498

     Mouse Winning Rate (from evaluation after training):

     Average success rate=0.144637

     Compare the final winning rate with that from Experiment 2, what can
     you say about the difference in winning rates?

     The initial success rate is lower than in 8X8 grid, but slowly grows in training.
     I guess this is because with a much larger QTable, 
     it takes more efforts to learn the entries.

6 .- (2 marks) Is standard Q-Learning a rasonable strategy for environments
     that change constantly? discuss based on the above

     No it is not reasonable for changing enviroments, because the trained 
     QTable entries won't fit very well in the new enviroments and the learnt 
     policies no longer help this case.

7 .- (5 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
	       to helping your mouse win

     I tried mutiple features including distances to varies objects, number of 
     threats(cats) and/or targets(cheeses) within specific distances, etc.

     But turned out they all doesn't work out very well as the manhatton distance 
     traps the mouse in a dead loop until the mouse eats it.

     Then I noticed the two headers defiend in QLearn.h:
     void expensiveFeature1(double gr[max_graph_size][4], int path[max_graph_size][2],int start_x, int start_y, int (*goalCheck)(int x, int y, int pos[5][2]), int pos[5][2], int s_type, int *l, int size_X);
     int checkForGoal(int x, int y, int pos[5][2]);

     I guess a wayyy better approach will be finding the **path** to the target and check that distance instead, and we may also compute the distances to threats for every step in the path. (hmm, for example: A* search?)

     But unfortunately I ran out of time...

     TAT

8 .- (2 marks each) Carry out the following experiments:

     # Experiment 6: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, one cat, and one cheese, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts):
     
     success rate=0.080514

     Mouse Winning Rate (from evaluation after training):

     Average success rate=0.056689

     Compare this rate against the rate obtained in Experiment #5, what
     are your conclusions regarding the utility of feature-based
     Q-Learning?

     Hmm, my feature set was not good... so no advantages could be seen...

     But I suppose even with a great set of features running feature-based and
     standard learning won't make too many differences **on a specific game setting**, either 
     feature-based learning or standard learning could do better than the other
     depending on the environment.

     # Experiment 7, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 4289, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training):

     Average success rate=0.061839

     # Experiment 8, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 31415, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training):

     Average success rate=0.080465

     Comparing the winning rates from Experiments #7 and #8 against
     those from Experiment #6, what can you conclude about feature-
     based Q-Learning in terms of its ability to deal with changes 
     to the environment?

     The rates didn't change too much among the 3 game environments, 
     feature-based learning should does better than QTable learning
     in dealing with environment changes.

9 .- (3 marks each) Carry out the following experiments:

     # Experiment 9: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, 2 cats, and 3 cheeses, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts):
     
     Mouse Winning Rate (from evaluation after training):
  
     # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 16x16 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training):

     # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 32x32 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training):

     Based on the last 3 experiments, what can you conclude regarding
     the usefulness and applicability of standard Q-Learning vs. 
     feature-based Q-learning?

     My code loops forever on the above settings...
     But theoretically speaking, with a good set of features that captures most of 
     the characteristics of the game state, feasture-based learning afvances over QTable learning since it adapts to changes more flexibly and actively.:)


***NOTE*** Greater than 1 cats' case does not work for me at all... I spent quite a few hours
detecting where the problem is, but seems the driver(core) simply doesn't call the 
action function after the update. 

_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.
	
			Complete/Working	Partial		Not done

QLearn  
 update              x

Reward
 function            x

Decide               x
 action

featureEval          x (not good... :(

evaluateQsa          x

maxQsa_prime         x

Qlearn_features      x

decideAction_features x

_____________________________________________________

Marking:

(5 marks) Implemented QLearn update.

(5 marks) Implemented a reasonable reward function

(5 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(15 marks) Implemented a non-trivial, good feature set
	   and evaluation function

(15 marks) Implemented a working feature-based Q-learning
	   algorithm

(20 marks) Competitive mouse performance

(35 marks) Answers in this report file

(- marks)  Penalty marks

Total for A4:       / out of 100


